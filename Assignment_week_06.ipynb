{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022620fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842f12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a classification dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d805fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.9736842105263158\n",
      "Precision: 0.9722222222222222\n",
      "Recall: 0.9859154929577465\n",
      "F1-Score: 0.9790209790209791\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.9649122807017544\n",
      "Precision: 0.958904109589041\n",
      "Recall: 0.9859154929577465\n",
      "F1-Score: 0.9722222222222222\n",
      "\n",
      "SVM Performance:\n",
      "Accuracy: 0.9824561403508771\n",
      "Precision: 0.9726027397260274\n",
      "Recall: 1.0\n",
      "F1-Score: 0.9861111111111112\n",
      "\n",
      "XGBoost Performance:\n",
      "Accuracy: 0.956140350877193\n",
      "Precision: 0.9583333333333334\n",
      "Recall: 0.971830985915493\n",
      "F1-Score: 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss')  # eval_metric needed for newer versions\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1-Score:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "325192f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (RF): {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Best F1 Score (RF): 0.9722550677337137\n"
     ]
    }
   ],
   "source": [
    "# Example: Tune Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='f1')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters (RF):\", grid_search_rf.best_params_)\n",
    "print(\"Best F1 Score (RF):\", grid_search_rf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c9ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters (XGB): {'subsample': 0.7, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "Best F1 Score (XGB): 0.9792195937429515\n"
     ]
    }
   ],
   "source": [
    "# Example: Tune XGBoost\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(XGBClassifier(eval_metric='logloss'), param_distributions=param_dist_xgb,\n",
    "                                       n_iter=10, scoring='f1', cv=5, random_state=42)\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters (XGB):\", random_search_xgb.best_params_)\n",
    "print(\"Best F1 Score (XGB):\", random_search_xgb.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85dae187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate tuned XGBoost\n",
    "best_xgb = random_search_xgb.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\nTuned XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94044e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "2                  SVM  0.982456   0.972603  1.000000  0.986111\n",
      "0  Logistic Regression  0.973684   0.972222  0.985915  0.979021\n",
      "1        Random Forest  0.964912   0.958904  0.985915  0.972222\n",
      "5        Tuned XGBoost  0.964912   0.958904  0.985915  0.972222\n",
      "3              XGBoost  0.956140   0.958333  0.971831  0.965035\n",
      "4  Tuned Random Forest  0.956140   0.958333  0.971831  0.965035\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    summary.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "# Add tuned models\n",
    "summary.append({\n",
    "    \"Model\": \"Tuned Random Forest\",\n",
    "    \"Accuracy\": accuracy_score(y_test, grid_search_rf.best_estimator_.predict(X_test)),\n",
    "    \"Precision\": precision_score(y_test, grid_search_rf.best_estimator_.predict(X_test)),\n",
    "    \"Recall\": recall_score(y_test, grid_search_rf.best_estimator_.predict(X_test)),\n",
    "    \"F1-Score\": f1_score(y_test, grid_search_rf.best_estimator_.predict(X_test)),\n",
    "})\n",
    "\n",
    "summary.append({\n",
    "    \"Model\": \"Tuned XGBoost\",\n",
    "    \"Accuracy\": accuracy_score(y_test, best_xgb.predict(X_test)),\n",
    "    \"Precision\": precision_score(y_test, best_xgb.predict(X_test)),\n",
    "    \"Recall\": recall_score(y_test, best_xgb.predict(X_test)),\n",
    "    \"F1-Score\": f1_score(y_test, best_xgb.predict(X_test)),\n",
    "})\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(df_summary.sort_values(by=\"F1-Score\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d9935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
